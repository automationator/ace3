import importlib
import sys
from typing import TYPE_CHECKING, Any, Optional
from pydantic import BaseModel, Field, ValidationError

if TYPE_CHECKING:
    from saq.configuration.yaml_parser import YAMLConfig
    from saq.modules.config import AnalysisModuleConfig

class GlobalConfig(BaseModel):
    company_name: str = Field(..., description="the default company name to use for alerts generated by engines using this configuration file")
    company_id: int = Field(..., description="the default company id to use for alerts generated by engines using this configuration file")
    instance_name: str = Field(..., description="shows up in the GUI so you know which instance you're logged into")
    node: str = Field(..., description="the name of this node, usually this is the fully qualified domain name; if this is set to AUTO then socket.getfqdn() is used")
    instance_type: str = Field(..., description="this can be either PRODUCTION, QA or DEV; used by some routines to prevent a user from making a horrible mistake")
    debug: bool = Field(..., description="forces the entire application to run under a single thread")
    data_dir: str = Field(..., description="global data directory that has all the things (relative to SAQ_HOME)")
    analyst_data_dir: str = Field(..., description="analyst data directory that has files that analysts can control through an external git repository")
    tmp_dir: Optional[str] = Field(default=None, description="if you need to override the default temp directory, set this to the path; if the path is relative, it is relative to the SAQ_HOME directory")
    error_reporting_dir: str = Field(..., description="directory that contains any stack traces for review (relative to DATA_DIR)")
    error_reporting_email: str = Field(..., description="list of email addresses (comma separated) that get these reports sent to them automatically")
    condition_reporting_delay: int = Field(..., description="the amount of time (in minutes) in between reporting of the same condition; set to 0 to disable condition reporting")
    local_domains: list[str] = Field(..., description="comma separated list of local domains")
    local_email_domains: list[str] = Field(..., description="comma separated list of valid email domains; these are domains that you receive emails for")
    log_sql: bool = Field(..., description="set to yes to log all SQL commands executed by the server")
    log_sql_exec_times: bool = Field(..., description="set to yes to log all SQL commands and their execution time")
    enable_semaphores: bool = Field(..., description="set this to True to enable semaphores; you'll definitely want this to be True in production settings")
    global_engine_instance_count: int = Field(..., description="set this to the total number of analysis engines (of any type) you have running")
    ignore_days: int = Field(..., description="the number of days alerts set to IGNORE will last until they are deleted from the system")
    fp_days: int = Field(..., description="the number of days alerts set to FALSE_POSITIVE will last until they are reset")
    distribute_days_old: int = Field(..., description="distribute old alerts that are NOT part of an event to another node after N days; set this to 0 to disable")
    distribution_target: str = Field(..., description="the node to distribute the alerts to; this will be equal to the 'node' value of this config section on the target machine")
    lock_timeout: str = Field(..., description="when alerts are being processed they are locked for processing; if a process dies during process then a stale lock could linger; the amount of time a lock is considered valid (in MM:SS format)")
    lock_keepalive_frequency: int = Field(..., description="amount of time (in seconds) between lock keep alive messages")
    maximum_cumulative_analysis_warning_time: int = Field(..., description="amount of time (in seconds) that we expect analysis to take, in general; we use this to warn ourselves that something might be wrong with logic in a module")
    maximum_cumulative_analysis_fail_time: int = Field(..., description="amount of time (in seconds) to give analysis (in total) before we bail entirely")
    maximum_analysis_time: int = Field(..., description="amount of time (in seconds) that we expect a single analysis module to take")
    check_watched_files_frequency: int = Field(..., description="how often (in seconds) modules check to see if watched files have been modified")
    memory_limit_warning: int = Field(..., description="issue a warning when a worker process uses this much memory (in MB)")
    memory_limit_kill: int = Field(..., description="kill a worker when it uses this much memory (in MB)")
    encrypted_passwords_db: str = Field(..., description="this is the name of the database configuration SECTION (database_SECTION) we use to query for encrypted passwords")
    maximum_analysis_disk_size: int = Field(..., description="maximum allowed size in bytes for an analysis (JSON + size of file observables); set this to 0 to disable this functionality")
    maximum_observable_count: int = Field(..., description="the total number of observables a single root analysis can have; once this threshold is met, no more observables can be added; set to 0 to disable this functionality")
    default_proxy: Optional[str] = Field(default=None, description="If set, all http/https requests will use this proxy by default")

class ServiceConfig(BaseModel):
    name: str = Field(..., description="The name of the service. Must be unique across all services.")
    python_module: str = Field(..., description="The Python module that contains the service class.")
    python_class: str = Field(..., description="The name of the service class inside the module.")
    description: str = Field(..., description="A brief description of the service.")
    enabled: bool = Field(..., description="Controls whether the service is enabled or disabled.")
    instance_types: Optional[list[str]] = Field(default=None, description="The instance types that the service is valid for.")

class DatabaseConfig(BaseModel):
    name: str = Field(..., description="The name of the database.")
    hostname: str = Field(..., description="The hostname of the database server.")
    port: int = Field(default=3306, description="The port of the database server.")
    unix_socket: Optional[str] = Field(default=None, description="(Optional) unix socket path.")
    database: str = Field(..., description="The name of the database.")
    username: str = Field(..., description="The username to use for authentication.")
    password: str = Field(..., description="The password to use for authentication.")
    max_allowed_packet: int = Field(default=1073741824, description="The maximum allowed packet size.")
    max_connection_lifetime: str = Field(default="00:15:00", description="The maximum connection lifetime (in HH:MM:SS format).")
    ssl_ca: Optional[str] = Field(default=None, description="(Optional) The path to the SSL CA certificate.")
    ssl_key: Optional[str] = Field(default=None, description="The path to the SSL key.")
    ssl_cert: Optional[str] = Field(default=None, description="The path to the SSL certificate.")

class ProxyConfig(BaseModel):
    name: str = Field(..., description="The name of the proxy.")
    transport: str = Field(..., description="The transport protocol to use for the proxy.")
    host: str = Field(..., description="The host of the proxy.")
    port: int = Field(..., description="The port of the proxy.")
    user: Optional[str] = Field(default=None, description="The username to use for authentication.")
    password: Optional[str] = Field(default=None, description="The password to use for authentication.")

class CollectionGroupConfig(BaseModel):
    name: str = Field(..., description="The name of the collection group.")
    enabled: bool = Field(..., description="Controls whether the collection group is enabled or disabled.")
    coverage: int = Field(..., description="The coverage (in percentage) of the collection group, ranging from 0 to 100.")
    full_delivery: bool = Field(..., description="Guarantees delivery of all submissions to this group.")
    database: str = Field(..., description="The database to use to find the target nodes to send submissions to.")
    company_id: int = Field(..., description="The company ID that the collection group belongs to.")
    target_nodes: Optional[list[str]] = Field(default=None, description="The target nodes to send submissions to.")
    thread_count: int = Field(default=1, description="The number of threads to use for the collection group.")

class LLMConfig(BaseModel):
    embedding_model: str = Field(..., description="the embedding model to use for vectorization")

class MonitorConfig(BaseModel):
    use_stdout: bool = Field(..., description="enable stdout monitoring")
    use_stderr: bool = Field(..., description="enable stderr monitoring")
    use_logging: bool = Field(..., description="enable logging monitoring")
    use_cache: bool = Field(..., description="enable cache monitoring")

class RabbitMQConfig(BaseModel):
    username: str = Field(..., description="rabbitmq username")
    password: str = Field(..., description="rabbitmq password")
    host: str = Field(..., description="rabbitmq host")
    port: int = Field(..., description="rabbitmq port")

class MinioConfig(BaseModel):
    host: str = Field(..., description="minio host")
    port: int = Field(..., description="minio port")
    access_key: str = Field(..., description="minio access key")
    secret_key: str = Field(..., description="minio secret key")
    secure: bool = Field(..., description="set to use SSL")
    cert_check: bool = Field(..., description="set to validate SSL certification")
    region: Optional[str] = Field(description="leave empty for MinIO server")

class RedisConfig(BaseModel):
    name: str = Field(..., description="redis name")
    host: str = Field(..., description="redis host")
    port: int = Field(..., description="redis port")
    username: str = Field(..., description="redis username")
    password: str = Field(..., description="redis password")
    use_ssl: bool = Field(..., description="use SSL for redis connection")
    ssl_ca_path: Optional[str] = Field(default=None, description="path to SSL CA certificate")

class QdrantConfig(BaseModel):
    url: str = Field(..., description="qdrant URL")
    use_ssl: bool = Field(..., description="use SSL for qdrant connection")
    ssl_ca_path: str = Field(..., description="path to SSL CA certificate")
    api_key: str = Field(..., description="qdrant API key")
    collection_alerts: str = Field(..., description="the collection name for ace3 alert data")

class SQLite3Config(BaseModel):
    timeout: int = Field(..., description="how long (in seconds) to wait for sqlite3 to connect")

class EncryptionConfig(BaseModel):
    salt_size: int = Field(..., description="size of the salt for PBKDF2 (in bytes)")
    iterations: int = Field(..., description="iterations for the PBKDF2 algorithm")

class CollectionConfig(BaseModel):
    persistence_dir: str = Field(..., description="contains various persistent information used by collectors (relative to DATA_DIR)")
    incoming_dir: str = Field(..., description="directory for incoming submissions (relative to DATA_DIR)")
    error_dir: str = Field(..., description="contains failed submission (relative to DATA_DIR)")
    #tuning_dir_default: str = Field(..., description="path (relative to SAQ_HOME) to directory that contains the yara rules used to tune collection")
    tuning_temp_dir: str = Field(..., description="path (relative to DATA_DIR) to use to store temporary file buffers for tuning")
    tuning_update_frequency: str = Field(..., description="control how often the tuning rules are checked for updates in HH:MM:SS format")
    force_api: bool = Field(..., description="set to yes to force collection to use the API even if the target node is local")
    tuning_dirs: list[str] = Field(..., description="list of directories that contain the yara rules used to tune collection")

class QueryHunterConfig(BaseModel):
    max_result_count: int = Field(..., description="maximum number of results queries should return")
    query_timeout: str = Field(..., description="maximum amount of time (in HH:MM:SS format) to wait for a query to complete")

class SSLConfig(BaseModel):
    ca_chain_path: str = Field(..., description="path to SSL CA chain certificate")

class APIConfig(BaseModel):
    listen_address: str = Field(..., description="binding address for incoming requests")
    listen_port: int = Field(..., description="listening port")
    ssl_cert: str = Field(..., description="path to SSL certificate")
    ssl_key: str = Field(..., description="path to SSL key")
    secret_key: str = Field(..., description="secret key for API")
    default_alert_type: str = Field(..., description="the default alert type when submissions do not include it")
    api_key: str = Field(..., description="autogenerated api key used for node-to-node communication")
    prefix: str = Field(..., description="the prefix that other systems use to connect to the API server")

class GUIConfig(BaseModel):
    listen_address: str = Field(..., description="binding address for incoming requests")
    listen_port: int = Field(..., description="listening port")
    ssl_cert: str = Field(..., description="path to SSL certificate")
    ssl_key: str = Field(..., description="path to SSL key")
    authentication: bool = Field(..., description="force users to authenticate to the gui")
    display_events: bool = Field(..., description="display events tab")
    display_metrics: bool = Field(..., description="display metrics tab")
    display_overview: bool = Field(..., description="display overview tab")
    google_analytics: bool = Field(..., description="enable Google Analytics")
    settings_import_enabled: bool = Field(..., description="allow settings to be imported")
    dispositioning: bool = Field(..., description="enable dispositioning feature")
    ownership: bool = Field(..., description="enable ownership feature")
    email_remediation: bool = Field(..., description="enable email remediation feature")
    event_management: bool = Field(..., description="enable event management feature")
    add_observable: bool = Field(..., description="enable add observable feature")
    analyze_alert: bool = Field(..., description="enable analyze alert feature")
    fp_easy_button: bool = Field(..., description="enable false positive easy button")
    ignore_easy_button: bool = Field(..., description="enable ignore easy button")
    upload_vt: bool = Field(..., description="enable VirusTotal upload")
    upload_vxstream: bool = Field(..., description="enable VxStream upload")
    view_in_vx: bool = Field(..., description="enable view in VxStream")
    clear_cloudphish_alert: bool = Field(..., description="allows analysts to clear cached URL content")
    show_total_alert_count: bool = Field(..., description="show the total alerts resulting from the current filter")
    matching_open_events_collapsed: bool = Field(..., description="default matching open events toggle position")
    alert_details_collapsed: bool = Field(..., description="default alert details toggle position")
    hide_intel: bool = Field(..., description="hide intel if we don't want to let people see it")
    base_uri: str = Field(..., description="the base address for the GUI (used to build ACE permalinks)")
    default_company_id: int = Field(..., description="default company for manual analysis")
    core_companies: list[int] = Field(..., description="define what companies are core companies")
    secret_key: str = Field(..., description="secret key used by flask")
    whitelist_excluded_observable_types: list[str] = Field(..., description="comma separated list of observable types that are excluded from whitelisting")
    file_preview_bytes: int = Field(..., description="specifies the number of bytes of read when generating previews for files")
    local_node_only: bool = Field(..., description="by default we only show alerts that exist on the local node")
    show_root_observables: bool = Field(..., description="enabling this option forces all observables in the root analysis to be visible in the critical analysis view")
    navigation_tabs: str = Field(..., description="a comma separated list of navigation tables visible from this node")
    display_node_list: Optional[list[str]] = Field(default=None, description="a comma separated list of nodes to display alerts for")

class NetworkConfigurationConfig(BaseModel):
    managed_networks: list[str] = Field(..., description="command separated list of CIDR notation for managed networks")

class WikiConfig(BaseModel):
    base_url: str = Field(..., description="base URL for wiki (designed to work with Confluence)")
    alert_reference_enabled: bool = Field(..., description="enable alert reference links to wiki documentation")
    event_link_enabled: bool = Field(..., description="enable wiki link next to event names on manage events page")

class SMTPConfig(BaseModel):
    enabled: bool = Field(..., description="enable SMTP email sending")
    server: str = Field(..., description="SMTP server address")
    mail_from: str = Field(..., description="from email address")

class MessagingConfig(BaseModel):
    lock_timeout: int = Field(..., description="how long (in seconds) a message can be locked before the lock times out")
    batch_size: int = Field(..., description="how many requests to lock for dispatch at a single time")

class EmailArchiveConfig(BaseModel):
    target: str = Field(..., description="possible values: minio, s3")
    primary: str = Field(..., description="if this system is archiving emails, this determines what section to use for the database config")
    s3_bucket: str = Field(..., description="the bucket to use for the email archive")
    s3_region: Optional[str] = Field(default=None, description="the region to use for the email archive")

class MemcachedConfig(BaseModel):
    client_address: str = Field(..., description="the address of the memcached system used by ACE")

class EmailConfig(BaseModel):
    email_dir: str = Field(..., description="the directory that contains the emails received by ACE for scanning (relative to DATA_DIR)")
    subdir_format: str = Field(..., description="the strftime format used to generate the subdirectory names that actually contain the emails")

class LDAPConfig(BaseModel):
    ldap_server: str = Field(..., description="the LDAP server (probably your domain controller)")
    ldap_port: int = Field(..., description="the port the LDAP server listens to")
    ldap_bind_user: str = Field(..., description="user account to use for authentication")
    ldap_bind_password: str = Field(..., description="LDAP password")
    ldap_base_dn: str = Field(..., description="the base DN for searching")
    top_user: str = Field(..., description="a user is tagged as executive if they are within 2 managers of this user")

class SplunkLoggingConfig(BaseModel):
    splunk_log_dir: str = Field(..., description="location of generated splunk logs (relative to DATA_DIR)")

class SplunkConfig(BaseModel):
    name: str = Field(..., description="the name of the splunk server")
    enabled: bool = Field(..., description="are we using this splunk server?")
    host: str = Field(..., description="the splunk query server")
    port: int = Field(..., description="the port of the splunk query server")
    username: Optional[str] = Field(default=None, description="user account information for splunk")
    password: Optional[str] = Field(default=None, description="password for authentication")
    token: Optional[str] = Field(default=None, description="token for authentication")
    timezone: str = Field(..., description="timezone for the splunk server")
    performance_logging_dir: str = Field(..., description="directory that contains splunk query performance data")
    user_context: Optional[str] = Field(default=None, description="default app and user context to use if not specified")
    app_context: Optional[str] = Field(default=None, description="default app and user context to use if not specified")
    proxy: Optional[str] = Field(default=None, description="optional proxy to use for splunk access")

class SplunkExportConfig(BaseModel):
    export_list: list[str] = Field(..., description="a comma separated list of for_detect observables types to export to splunk lookup table")
    max_export: int = Field(..., description="the maximum number of observables to export in each request")
    api: str = Field(..., description="the splunk config name to use for the API")
    user_context: Optional[str] = Field(default=None, description="user context to use if not specified")
    app: Optional[str] = Field(default=None, description="app context to use if not specified")

class SIPConfig(BaseModel):
    enabled: bool = Field(..., description="enable SIP integration")
    remote_address: str = Field(..., description="SIP remote address")
    api_key: str = Field(..., description="SIP API key")
    cache_db_path: str = Field(..., description="path to SIP cache database")

class ShodanConfig(BaseModel):
    api_key: Optional[str] = Field(default=None, description="Shodan API key")
    delay: int = Field(default=1, description="how long to sleep before making the next query (see the rate limit comment)")

class YaraExportConfig(BaseModel):
    export_list: list[str] = Field(..., description="list of for_detect observables types to export to yara rules")
    export_template_dir: str = Field(..., description="directory of template files (relative to ANALYST_DATA_DIR)")
    export_minimum_length: int = Field(..., description="minimum length of an indicator value that would go into a yara rule")
    max_strings_per_rule: int = Field(..., description="max number of strings to allow in a yara rule")

class SIPYaraExportConfig(BaseModel):
    export_list: list[str] = Field(..., description="list of sip indicator types to export to yara rules")
    export_sources_include: str = Field(..., description="list of sources to include (defaults to all sources if left empty)")
    export_sources_exclude: str = Field(..., description="list of sources to exclude (defaults to none if left empty)")
    export_template_dir: str = Field(..., description="directory of template files")
    export_minimum_length: int = Field(..., description="minimum length of an indicator value that would go into a yara rule")

class MaliciousFilesConfig(BaseModel):
    malicious_dir: str = Field(..., description="directory where malicious files are stored")
    malicious_alert_recipients: list[str] = Field(..., description="comma separated list of email accounts to send notifications to when new files are added")

class CustomAlertsConfig(BaseModel):
    template_dir: str = Field(..., description="directory containing all flask views")
    dirs: list[str] = Field(..., description="list of dirs containing all custom alert views")

class DomainGenerationConfig(BaseModel):
    keyword_file_path: str = Field(..., description="path to keyword file (relative to ANALYST_DATA_DIR)")
    opensquat: str = Field(..., description="relative path to the opensquat external tool")
    static_domain_file: str = Field(..., description="path to static domains file (relative to ANALYST_DATA_DIR)")

class SettingsConfig(BaseModel):
    refresh_seconds: int = Field(..., description="how often to refresh settings (in seconds)")

class ObservableExpirationMappingsConfig(BaseModel):
    never_expire_with_threat_actor: bool = Field(..., description="set any observables expires_on value to Null (never expire) if the observable is inside an event that has a threat actor assigned to it")

class EventsConfig(BaseModel):
    closed_status: str = Field(..., description="the event status that you use to indicate that the event is closed or fully processed")
    autoclose_path: str = Field(..., description="file name that contains the autoclose criteria (path is relative to ANALYST_DATA_DIR)")

class TimelineConfig(BaseModel):
    timezone: str = Field(..., description="timezone for timeline display")

#class ConfigConfig(BaseModel):
    #git_repos_default: str = Field(..., description="path to default git repos configuration")
    #signatures_default: str = Field(..., description="path to default signatures configuration")
    #zeek_default: str = Field(..., description="path to default zeek configuration")
    #remediation_default: str = Field(..., description="path to default remediation configuration")
    #shodan_default: str = Field(..., description="path to default shodan configuration")
    #splunk_default: str = Field(..., description="path to default splunk configuration")

class IntegrationConfig(BaseModel):
    name: str = Field(..., description="the name of the integration")
    description: str = Field(..., description="the description of the integration")
    enabled: bool = Field(..., description="whether the integration is enabled or disabled")
    python_module: str = Field(..., description="the python module of the integration")

class AnalysisModuleGroupConfig(BaseModel):
    name: str = Field(..., description="the name of the analysis module group")
    modules: list[str] = Field(..., description="the list of analysis modules in the group referenced by name")

class AnalysisModeConfig(BaseModel):
    name: str = Field(..., description="the name of the analysis mode")
    enabled_modules: Optional[list[str]] = Field(default_factory=list, description="the list of enabled modules in the analysis mode")
    disabled_modules: Optional[list[str]] = Field(default_factory=list, description="the list of disabled modules in the analysis mode")
    module_groups: list[str] = Field(..., description="the list of module groups in the analysis mode")
    cleanup: bool = Field(..., description="whether the analysis mode should clean up after itself")
    maximum_cumulative_analysis_warning_time: Optional[int] = Field(default=None, description="the maximum cumulative analysis warning time (in seconds)")
    maximum_cumulative_analysis_fail_time: Optional[int] = Field(default=None, description="the maximum cumulative analysis fail time (in seconds)")
    maximum_analysis_time: Optional[int] = Field(default=None, description="the maximum analysis time (in seconds)")

class HuntTypeConfig(BaseModel):
    name: str = Field(..., description="The name of the hunt type.")
    python_module: str = Field(..., description="The module of the hunt type.")
    python_class: str = Field(..., description="The class of the hunt type.")
    rule_dirs: list[str] = Field(..., description="The directories that contain the hunt rules.")
    update_frequency: int = Field(..., description="The frequency of the hunt type.")
    concurrency_limit: Optional[int] = Field(default=None, description="The concurrency limit for the hunt type.")

class APIQueryDefaultsConfig(BaseModel):
    name: str = Field(..., description="The name of the API query defaults.")
    wide_duration_before: str = Field(..., description="The wide duration before the analysis.")
    wide_duration_after: str = Field(..., description="The wide duration after the analysis.")
    narrow_duration_before: str = Field(..., description="The narrow duration before the analysis.")
    narrow_duration_after: str = Field(..., description="The narrow duration after the analysis.")
    max_result_count: int = Field(..., description="The max result count for the analysis.")
    query_timeout: int = Field(..., description="The query timeout for the analysis.")
    async_delay: int = Field(..., description="The async delay for the analysis.")
    use_index_time: bool = Field(..., description="Whether to use the index time as the time of the query.")

class GitRepoConfig(BaseModel):
    name: str = Field(..., description="The name of the repo.")
    description: str = Field(..., description="The description of the repo.")
    local_path: str = Field(..., description="The local path to the repo.")
    git_url: str = Field(..., description="The git url of the repo.")
    update_frequency: int = Field(..., description="The frequency to update the repo in seconds.")
    branch: str = Field(..., description="The branch to use for the repo.")
    ssh_key_path: Optional[str] = Field(default=None, description="The path to an ssh key to use for the repo.")

class RemediatorConfig(BaseModel):
    name: str = Field(..., description="The name of the remediator.")
    display_name: str = Field(..., description="The display name of the remediator.")
    description: str = Field(..., description="The description of the remediator.")
    observable_type: str = Field(..., description="The observable type the remediator works on.")
    python_module: str = Field(..., description="The module of the remediator.")
    python_class: str = Field(..., description="The class of the remediator.")
    thread_count: int = Field(default=1, description="The maximum number of threads for the remediator.")


class FileCollectorConfig(BaseModel):
    """Configuration for a file collector implementation."""
    name: str = Field(..., description="The name of the file collector.")
    display_name: str = Field(..., description="The display name of the file collector.")
    description: str = Field(..., description="The description of the file collector.")
    observable_type: str = Field(..., description="The observable type the collector works on (e.g., file_location).")
    python_module: str = Field(..., description="The module of the file collector.")
    python_class: str = Field(..., description="The class of the file collector.")
    thread_count: int = Field(default=1, description="The maximum number of threads for the collector.")
    initial_retry_delay_seconds: int = Field(
        default=60,
        description="Initial delay between collection retry attempts."
    )
    max_retry_delay_seconds: int = Field(
        default=3600,
        description="Maximum delay between retry attempts (1 hour default)."
    )
    max_collection_time_seconds: int = Field(
        default=604800,
        description="Maximum total time to keep retrying collection (7 days default)."
    )


class ACEConfig(BaseModel):
    global_settings: GlobalConfig = Field(alias="global")
    llm: Optional[LLMConfig] = None
    monitor: Optional[MonitorConfig] = None
    rabbitmq: Optional[RabbitMQConfig] = None
    minio: Optional[MinioConfig] = None
    redis: Optional[RedisConfig] = None
    redis_local: Optional[RedisConfig] = Field(default=None, alias="redis-local")
    qdrant: Optional[QdrantConfig] = None
    database: Optional[DatabaseConfig] = None
    sqlite3: Optional[SQLite3Config] = None
    encryption: Optional[EncryptionConfig] = None
    collection: Optional[CollectionConfig] = None
    query_hunter: Optional[QueryHunterConfig] = None
    node_translation: Optional[dict[str, str]] = None
    node_translation_gui: Optional[dict[str, str]] = None
    SSL: Optional[SSLConfig] = None
    api: Optional[APIConfig] = None
    apikeys: Optional[dict[str, str]] = None
    gui: Optional[GUIConfig] = None
    gui_favicons: Optional[dict[str, str]] = None
    network_configuration: Optional[NetworkConfigurationConfig] = None
    wiki: Optional[WikiConfig] = None
    smtp: Optional[SMTPConfig] = None
    messaging: Optional[MessagingConfig] = None
    message_routing: Optional[dict] = None
    email_archive: Optional[EmailArchiveConfig] = None
    memcached: Optional[MemcachedConfig] = None
    email: Optional[EmailConfig] = None
    ldap: Optional[LDAPConfig] = None
    splunk_logging: Optional[SplunkLoggingConfig] = None
    splunk_export: Optional[SplunkExportConfig] = None
    sip: Optional[SIPConfig] = None
    shodan: Optional[ShodanConfig] = None
    yara_export: Optional[YaraExportConfig] = None
    yara_export_string_modifiers: Optional[dict[str, str]] = None
    sip_yara_export: Optional[SIPYaraExportConfig] = None
    sip_yara_export_string_modifiers: Optional[dict[str, str]] = None
    sip_indicator_type_mapping: Optional[dict[str, str]] = None
    sip_observable_type_mappping: Optional[dict[str, str]] = None
    malicious_files: Optional[MaliciousFilesConfig] = None
    observable_exclusions: Optional[dict[str, str]] = None
    custom_alerts: Optional[CustomAlertsConfig] = None
    custom_alerts_backward_compatibility: Optional[dict[str, str]] = None
    disabled_modules: Optional[list[str]] = Field(default_factory=list, description="list of analysis modules that should be disabled globally")
    tags: Optional[dict[str, str]] = None
    tag_css_class: Optional[dict[str, str]] = None
    domain_generation: Optional[DomainGenerationConfig] = None
    settings: Optional[SettingsConfig] = None
    observable_expiration_mappings: dict[str, str] = Field(default_factory=dict, description="dictionary of observable types and their expiration mappings")
    events: Optional[EventsConfig] = None
    timeline: Optional[TimelineConfig] = None
    valid_dispositions: Optional[dict[str, bool]] = None
    disposition_rank: Optional[dict[str, int]] = None
    disposition_css: Optional[dict[str, str]] = None
    show_save_to_event: Optional[dict[str, bool]] = None
    benign_dispositions: Optional[dict[str, bool]] = None
    malicious_dispositions: Optional[dict[str, bool]] = None

    def model_post_init(self, __context: Any) -> None:
        """Initialize private fields after model validation."""
        self.__raw: Optional["YAMLConfig"] = None
        self.__integrations: Optional[dict[str, IntegrationConfig]] = None
        self.__databases: Optional[dict[str, DatabaseConfig]] = None
        self.__services: Optional[dict[str, ServiceConfig]] = None
        self.__analysis_modules: Optional[dict[str, "AnalysisModuleConfig"]] = None
        self.__analysis_module_groups: Optional[dict[str, "AnalysisModuleGroupConfig"]] = None
        self.__analysis_modes: Optional[dict[str, AnalysisModeConfig]] = None
        self.__collection_groups: Optional[dict[str, CollectionGroupConfig]] = None
        self.__proxies: Optional[dict[str, ProxyConfig]] = None
        self.__splunk_configs: Optional[dict[str, SplunkConfig]] = None
        self.__api_query_defaults: Optional[dict[str, APIQueryDefaultsConfig]] = None
        self.__hunt_types: Optional[dict[str, HuntTypeConfig]] = None
        self.__git_repos: Optional[dict[str, GitRepoConfig]] = None
        self.__remediators: Optional[dict[str, RemediatorConfig]] = None
        self.__file_collectors: Optional[dict[str, FileCollectorConfig]] = None

    def resolve_all_values(self):
        self.__raw.resolve_all_values()
        self.parse_raw_data()

    #
    # raw data parsing
    #

    def _raise_raw_data_error(self):
        raise RuntimeError("Raw data not initialized")

    @property
    def raw(self) -> "YAMLConfig":
        """Property to access raw data that is not part of model validation."""
        if self.__raw is None:
            self._raise_raw_data_error()

        return self.__raw

    @raw.setter
    def raw(self, value: "YAMLConfig"):
        """Setter for raw data property."""
        self.__raw = value
        self.parse_raw_data()

    def parse_raw_data(self):
        self.load_integration_configs()
        self.load_database_configs()
        self.load_service_configs()
        self.load_analysis_mode_configs()
        self.load_analysis_module_configs()
        self.load_module_group_configs()
        self.load_collection_group_configs()
        self.load_proxy_configs()
        self.load_splunk_configs()
        self.load_hunt_type_configs()
        self.load_api_query_defaults_config()
        self.load_git_repo_configs()
        self.load_remediator_configs()
        self.load_file_collector_configs()
    #
    # integrations
    #

    def load_integration_configs(self):
        """Loads the integration configurations."""
        self.__integrations = {}

        for key, value in self.raw._data.items():
            if not key.startswith("integration_"):
                continue

            integration_name = key[len("integration_"):]

            integration_config = IntegrationConfig.model_validate(value)
            self.__integrations[integration_name] = integration_config

    @property
    def integrations(self) -> list[IntegrationConfig]:
        """Property to access the integrations configurations."""
        if self.__integrations is None:
            self._raise_raw_data_error()

        return self.__integrations.values()

    #
    # databases
    #

    @property
    def databases(self) -> list[DatabaseConfig]:
        if self.__databases is None:
            self._raise_raw_data_error()

        return self.__databases.values()

    def load_database_configs(self):
        self.__databases = {}

        for key, value in self.raw._data.items():
            if not key.startswith("database_"):
                continue

            database_config = DatabaseConfig.model_validate(value)
            self.add_database_config(database_config.name, database_config)

    def add_database_config(self, name: str, database_config: DatabaseConfig):
        self.__databases[name] = database_config

    def get_database_config(self, name: str) -> DatabaseConfig:
        if self.__databases is None:
            self._raise_raw_data_error()

        if name not in self.__databases:
            raise ValueError(f"database config for {name} not found")

        return self.__databases[name]

    #
    # services
    #

    @property
    def services(self) -> list[ServiceConfig]:
        if self.__services is None:
            self._raise_raw_data_error()

        return self.__services.values()

    def add_service_config(self, name: str, service_config: ServiceConfig):
        self.__services[name] = service_config

    def load_service_configs(self):
        self.__services = {}

        for key, value in self.raw._data.items():
            if not key.startswith("service_"):
                continue

            try:
                service_config = ServiceConfig.model_validate(value)
            except Exception as e:
                sys.stderr.write(f"CONFIG ERROR: failed to validate service config for {key}\n")
                raise e

            # load the service class so we can figure out what config class to use
            module = importlib.import_module(service_config.python_module)
            class_definition = getattr(module, service_config.python_class)

            try:
                self.add_service_config(service_config.name, class_definition.get_config_class().model_validate(value))
            except Exception as e:
                sys.stderr.write(f"CONFIG ERROR: failed to validate service config for {key}\n")
                raise e

    def get_service_config(self, name: str) -> ServiceConfig:
        if self.__services is None:
            self._raise_raw_data_error()

        if name not in self.__services:
            raise ValueError(f"service config for {name} not found")

        return self.__services[name]

    #
    # analysis modules
    #

    @property
    def analysis_modules(self) -> list["AnalysisModuleConfig"]:
        if self.__analysis_modules is None:
            self._raise_raw_data_error()

        return self.__analysis_modules.values()

    def add_analysis_module_config(self, name: str, analysis_module_config: "AnalysisModuleConfig"):
        self.__analysis_modules[name] = analysis_module_config

    def load_analysis_module_configs(self):
        from saq.modules.config import AnalysisModuleConfig
        self.__analysis_modules = {}

        for key, value in self.raw._data.items():
            if not key.startswith("analysis_module_"):
                continue

            try:
                analysis_module_config = AnalysisModuleConfig.model_validate(value)
            except Exception as e:
                sys.stderr.write(f"CONFIG ERROR: failed to validate analysis module config for {key}\n")
                raise e

            # load the analysis module class so we can figure out what config class to use
            module = importlib.import_module(analysis_module_config.python_module)
            class_definition = getattr(module, analysis_module_config.python_class)

            try:
                self.add_analysis_module_config(analysis_module_config.name, class_definition.get_config_class().model_validate(value))
            except Exception as e:
                sys.stderr.write(f"CONFIG ERROR: failed to validate analysis module config for {key}\n")
                raise e
    
    def get_analysis_module_config(self, name: str) -> "AnalysisModuleConfig":
        if self.__analysis_modules is None:
            self._raise_raw_data_error()

        if name not in self.__analysis_modules:
            raise ValueError(f"analysis module config for {name} not found")

        return self.__analysis_modules[name]

    # 
    # analysis modes
    #

    @property
    def analysis_modes(self) -> list[AnalysisModeConfig]:
        if self.__analysis_modes is None:
            self._raise_raw_data_error()

        return self.__analysis_modes.values()

    def add_analysis_mode_config(self, name: str, analysis_mode_config: AnalysisModeConfig):
        self.__analysis_modes[name] = analysis_mode_config

    def load_analysis_mode_configs(self):
        self.__analysis_modes = {}

        for key, value in self.raw._data.items():
            if not key.startswith("analysis_mode_"):
                continue

            try:
                analysis_mode_config = AnalysisModeConfig.model_validate(value)
            except ValidationError as e:
                sys.stderr.write(f"CONFIG ERROR: failed to validate analysis mode config for {key}\n")
                sys.stderr.write(str(value))
                raise e

            self.add_analysis_mode_config(analysis_mode_config.name, analysis_mode_config)

    def get_analysis_mode_config(self, name: str) -> AnalysisModeConfig:
        if self.__analysis_modes is None:
            self._raise_raw_data_error()

        if name not in self.__analysis_modes:
            raise ValueError(f"analysis mode config for {name} not found")

        return self.__analysis_modes[name]
    
    #
    # module groups
    #

    @property
    def module_groups(self) -> list[AnalysisModuleGroupConfig]:
        if self.__module_groups is None:
            self._raise_raw_data_error()

        return self.__module_groups.values()
    
    def load_module_group_configs(self):
        self.__module_groups = {}

        for key, value in self.raw._data.items():
            if not key.startswith("module_group_"):
                continue

            module_group_name = key[len("module_group_"):]
            module_group_config = AnalysisModuleGroupConfig.model_validate(value)
            self.add_module_group_config(module_group_name, module_group_config)

    def add_module_group_config(self, name: str, module_group_config: AnalysisModuleGroupConfig):
        self.__module_groups[name] = module_group_config

    def get_module_group_config(self, name: str) -> AnalysisModuleGroupConfig:
        if self.__module_groups is None:
            self._raise_raw_data_error()

        if name not in self.__module_groups:
            raise ValueError(f"module group config for {name} not found")

        return self.__module_groups[name]

    #
    # collection groups
    #

    @property
    def collection_groups(self) -> list[CollectionGroupConfig]:
        if self.__collection_groups is None:
            self._raise_raw_data_error()

        return self.__collection_groups.values()

    def add_collection_group_config(self, name: str, collection_group_config: CollectionGroupConfig):
        self.__collection_groups[name] = collection_group_config

    def clear_collection_group_configs(self):
        self.__collection_groups = {}
    
    def load_collection_group_configs(self):
        self.__collection_groups = {}

        for key, value in self.raw._data.items():
            if not key.startswith("collection_group_"):
                continue

            collection_group_name = key[len("collection_group_"):]
            collection_group_config = CollectionGroupConfig.model_validate(value)
            self.add_collection_group_config(collection_group_name, collection_group_config)

    def get_collection_group_config(self, name: str) -> CollectionGroupConfig:
        if self.__collection_groups is None:
            self._raise_raw_data_error()

        if name not in self.__collection_groups:
            raise ValueError(f"collection group config for {name} not found")

        return self.__collection_groups[name]

    #
    # proxies
    #

    @property
    def proxies(self) -> list[ProxyConfig]:
        if self.__proxies is None:
            self._raise_raw_data_error()

        return self.__proxies.values()

    def add_proxy_config(self, name: str, proxy_config: ProxyConfig):
        self.__proxies[name] = proxy_config

    def clear_proxy_configs(self):
        self.__proxies = {}

    def load_proxy_configs(self):
        self.__proxies = {}

        for key, value in self.raw._data.items():
            if not key.startswith("proxy_"):
                continue

            proxy_name = key[len("proxy_"):]
            proxy_config = ProxyConfig.model_validate(value)
            self.add_proxy_config(proxy_name, proxy_config)

    def get_proxy_config(self, name: Optional[str] = None) -> ProxyConfig:
        if self.__proxies is None:
            self._raise_raw_data_error()

        if name is None:
            name = self.global_settings.default_proxy

        if name is None:
            raise ValueError("no proxy is configured")

        if name not in self.__proxies:
            raise ValueError(f"proxy config for {name} not found")

        return self.__proxies[name]

    #
    # splunk configs
    #

    @property
    def splunk_configs(self) -> dict[str, SplunkConfig]:
        if self.__splunk_configs is None:
            self._raise_raw_data_error()

        return self.__splunk_configs.values()

    def add_splunk_config(self, name: str, splunk_config: SplunkConfig):
        self.__splunk_configs[name] = splunk_config

    def clear_splunk_configs(self):
        self.__splunk_configs = {}
    
    def load_splunk_configs(self):
        self.__splunk_configs = {}

        for key, value in self.raw._data.items():
            if not key.startswith("splunk_config_"):
                continue

            splunk_name = key[len("splunk_config_"):]
            splunk_config = SplunkConfig.model_validate(value)
            self.add_splunk_config(splunk_name, splunk_config)

    def get_splunk_config(self, name: str = "default") -> SplunkConfig:
        if self.__splunk_configs is None:
            self._raise_raw_data_error()

        if name not in self.__splunk_configs:
            raise ValueError(f"splunk config for {name} not found")

        return self.__splunk_configs[name]

    #
    # api query defaults
    #

    @property
    def api_query_defaults(self) -> list[APIQueryDefaultsConfig]:
        if self.__api_query_defaults is None:
            self._raise_raw_data_error()

        return self.__api_query_defaults.values()
    
    def load_api_query_defaults_config(self):
        self.__api_query_defaults = {}

        for key, value in self.raw._data.items():
            if not key.startswith("api_query_defaults_"):
                continue

            api_query_defaults_name = key[len("api_query_defaults_"):]
            api_query_defaults_config = APIQueryDefaultsConfig.model_validate(value)
            self.add_api_query_defaults_config(api_query_defaults_name, api_query_defaults_config)

    def add_api_query_defaults_config(self, name: str, api_query_defaults_config: APIQueryDefaultsConfig):
        self.__api_query_defaults[name] = api_query_defaults_config

    def get_api_query_defaults_config(self, name: str) -> APIQueryDefaultsConfig:
        if self.__api_query_defaults is None:
            self._raise_raw_data_error()

        if name not in self.__api_query_defaults:
            raise ValueError(f"api query defaults config for {name} not found")

        return self.__api_query_defaults[name]

    #
    # hunt types
    #

    @property
    def hunt_types(self) -> list[HuntTypeConfig]:
        if self.__hunt_types is None:
            self._raise_raw_data_error()

        return self.__hunt_types.values()

    def clear_hunt_type_configs(self):
        self.__hunt_types = {}

    def add_hunt_type_config(self, name: str, hunt_type_config: HuntTypeConfig):
        self.__hunt_types[name] = hunt_type_config
    
    def load_hunt_type_configs(self):
        self.__hunt_types = {}

        for key, value in self.raw._data.items():
            if not key.startswith("hunt_type_"):
                continue

            hunt_type_name = key[len("hunt_type_"):]
            hunt_type_config = HuntTypeConfig.model_validate(value)
            self.add_hunt_type_config(hunt_type_name, hunt_type_config)

    def get_hunt_type_config(self, name: str) -> HuntTypeConfig:
        if self.__hunt_types is None:
            self._raise_raw_data_error()

        if name not in self.__hunt_types:
            raise ValueError(f"hunt type config for {name} not found")

        return self.__hunt_types[name]

    #
    # git repos
    #

    @property
    def git_repos(self) -> list[GitRepoConfig]:
        if self.__git_repos is None:
            self._raise_raw_data_error()

        return self.__git_repos.values()
    
    def clear_git_repo_configs(self):
        self.__git_repos = {}
    
    def load_git_repo_configs(self):
        self.__git_repos = {}

        for key, value in self.raw._data.items():
            if not key.startswith("git_repo_"):
                continue

            git_repo_name = key[len("git_repo_"):]
            git_repo_config = GitRepoConfig.model_validate(value)
            self.add_git_repo_config(git_repo_name, git_repo_config)

    def add_git_repo_config(self, name: str, git_repo_config: GitRepoConfig):
        self.__git_repos[name] = git_repo_config

    def get_git_repo_config(self, name: str) -> GitRepoConfig:
        if self.__git_repos is None:
            self._raise_raw_data_error()

        if name not in self.__git_repos:
            raise ValueError(f"git repo config for {name} not found")

        return self.__git_repos[name]

    #
    # remediators
    #

    @property
    def remediators(self) -> list[RemediatorConfig]:
        if self.__remediators is None:
            self._raise_raw_data_error()

        return self.__remediators.values()
    
    def load_remediator_configs(self):
        self.__remediators = {}

        for key, value in self.raw._data.items():
            if not key.startswith("remediator_"):
                continue

            remediator_config = RemediatorConfig.model_validate(value)
            self.add_remediator_config(remediator_config.name, remediator_config)
    
    def add_remediator_config(self, name: str, remediator_config: RemediatorConfig):
        self.__remediators[name] = remediator_config

    def get_remediator_config(self, name: str) -> RemediatorConfig:
        if self.__remediators is None:
            self._raise_raw_data_error()

        if name not in self.__remediators:
            raise ValueError(f"remediator config for {name} not found")

        return self.__remediators[name]

    #
    # file collectors
    #

    @property
    def file_collectors(self) -> list[FileCollectorConfig]:
        if self.__file_collectors is None:
            self._raise_raw_data_error()

        return self.__file_collectors.values()

    def load_file_collector_configs(self):
        self.__file_collectors = {}

        for key, value in self.raw._data.items():
            if not key.startswith("file_collector_"):
                continue

            file_collector_config = FileCollectorConfig.model_validate(value)
            self.add_file_collector_config(file_collector_config.name, file_collector_config)

    def add_file_collector_config(self, name: str, file_collector_config: FileCollectorConfig):
        self.__file_collectors[name] = file_collector_config

    def get_file_collector_config(self, name: str) -> FileCollectorConfig:
        if self.__file_collectors is None:
            self._raise_raw_data_error()

        if name not in self.__file_collectors:
            raise ValueError(f"file collector config for {name} not found")

        return self.__file_collectors[name]